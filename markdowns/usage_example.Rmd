---
title: "Usage example"
author: "David Quesada"
date: "25/10/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This is a full example of the basic usage of the package, from learning the DBN network structure to forecasting.

## Example dataset

We will use the sample of the 'motor' dataset included in the package for the example, consisting of 3000 instances and 12 variables. The training times vary greatly depending on the size of the networks trained and the size of the dataset. This dataset is only intended to perform some tests with the package and show the functionallity, it is expected that we will model it poorly, given that this is only a minimal part of the original dataset. The original dataset can be found at <https://www.kaggle.com/wkirgsn/electric-motor-temperature>.

```{r motor}
library(dbnR)
library(data.table)
data(motor)
summary(motor)
```

## Net stucture learning

To learn the structure of a DBN, first we have to fix its size and set some parameters for the learning algorithm. If the 'dmmhc' method is selected, further parameters can be given to the algorithm that will be passed down to the rsmax2 function from the bnlearn package. Two specially usefull parameters are 'maxp' inside 'maximize.args', which sets the maximum number of parents that a node can have, and 'blacklist', which allows us to forbid arcs in the static structure of the network. 

```{r structure}
size = 3
dt_train <- motor[1:2800]
dt_val <- motor[2801:3000]
blacklist <- c("motor_speed_t_0", "i_d_t_0")
blacklist <- rbind(blacklist, c("motor_speed_t_0", "i_q_t_0"))
net <- dbnR::learn_dbn_struc(dt_train, size, method = "dmmhc", blacklist = blacklist,
                             restrict = "mmpc", maximize = "hc", 
                             restrict.args = list(test = "cor"),
                             maximize.args = list(score = "bic-g", maxp = 10))

#net <- dbnR::learn_dbn_struc(dt_train, size, method = "psoho")
```

The 'blacklist' parameter is a 2 column matrix where the first column is the 'from' and the second is the 'to'. It's important to append the "_t_0" to the names of the nodes, because they are renamed inside the algorithm to that format. A 'whitelist' parameter can also be provided to force some arcs to be present with the same format as the blacklist, but it can sometimes increase the execution time greatly. Only include arcs in t_0 in both the blacklist and whitelist, for now those two are only available for the static network learning, not the transition one. 

Another interesting parameter is 'f_dt'. It allows the user to provide an already folded dataset, mainly so that some instances can be removed. We can also avoid learning intra-slice arcs with the 'intra' parameter: if TRUE, intra-slice arcs will be learnt as normal, but if it is set to FALSE we can skip the learning of the static structure in the DBN.

The particle swarm optimization for higher-order DBNs (psoho) algorithm can also be used to learn the DBN structure. Parameters such as the number of particles and the number of iterations can be tuned. It returns a specific type of structures where no intra-slice arcs are present and the only arcs allowed are those that end in t_0. With the default parameters, it takes quite longer than the dmmhc algorithm to learn the structure on small datasets, but the results are sometimes more interesting. On bigger datasets, the execution time of both algorithms is very similar.

## Parameter learning

Once we have the structure of the DBN, we have to fold our dataset and fit the parameters of the DBN:

```{r params}
f_dt_train <- fold_dt(dt_train, size)
f_dt_val <- fold_dt(dt_val, size)
fit <- dbnR::fit_dbn_params(net, f_dt_train, method = "mle")
```

## Visualization

We have implemented a visualization tool for the DBN structures. It will plot an interactive html graph via the 'visNetwork' package. Each time slice is displayed in a different color and the nodes can be selected and moved around to see the local structures better. Both the network and the fit objects can be plotted.

```{r vis}
dbnR::plot_dynamic_network(fit)
```

## Inference and forecasting

After fitting the model, we can perform inference over it. One of the particularities of BNs is that any variable can be the objective of the inference, so we have to decide which variables are going to be the objective ones. There are two types of inference available: point-wise inference and forecasting. Point-wise inference will give a prediction for the objective variables in each row of the folded dataset. Forecasting will give the predicted values of the objective variable over a span of time from a starting point.

```{r one_inference_step}
obj_var <- c("stator_winding_t_0")
res <- (dbnR::predict_dt(fit, f_dt_val, obj_var))
```

One should be careful when performing point-wise prediction on time-series models, because a very good prediction of the future time step is the previous value. This means that sometimes a time-series model can be just passing forward the values of the variables from the previous step, resulting in very good accuracy when the forecasting horizon is 1 instant, but performing very poorly as the horizon increases. I've seen this behaviour with multiple time-series models, so one should be weary of this. In our example, the predictions look deceptively good, but in reality the model does not forecast properly because we extracted only a minimal part of the original dataset for testing purposes, not forecasting accuracy.

Next, we will forecast some of the variables in the validation dataset:

```{r forecast}
res_fore <- suppressWarnings(dbnR::forecast_ts(f_dt_val, fit, size, obj_var = c("pm_t_0", "stator_winding_t_0"), ini = 100, len = 70))
set.seed(42)
res_fore <- suppressWarnings(dbnR::forecast_ts(f_dt_val, fit, size, obj_var = c("pm_t_0"), ini = 100, len = 70, mode = "approx", num_p = 150, rep = 3))
```

With DBNs, all variables in the next time step are predicted from the values in the previous slices. If no evidence is provided, all the variables in t_0 will be forecasted and moved to t_1 in the next prediction step. The 'obj_var' parameter is used to return the forecasting results of the variables we are interested in. The forecasting length and starting point can be modified as showed in the example.

There are two forecasting modes: exact and approximate inference. Exact inference is faster and more reliable than the approximate particle inference in the scenario with only continuous variables, so it should be prioritized. It is also important to note that the exact inference shows the mean values of the nodes in each predicted step, and it is expected some variance that is not shown in the plot. The predictions are so smooth because of this, and if the approximate inference is used this variance can be seen by running the inference several times in a row. It remains as future work to show the expected variance of the forecasting in the plot.

We can also perform evidenced forecasting with our models. We call 'evidenced forecasting' to the special case when we already know the values that some variables are going to take in the future, and so we set those values and predict the rest. This is very useful if we want our models to serve as simulators, fixing some variables to certain values to see the effect that those have on the forecasting.

```{r forecast_ev_1}
res_fore <- suppressWarnings(dbnR::forecast_ts(f_dt_val, fit, size, obj_var = c("pm_t_0"), ini = 100, len = 70, prov_ev = c("stator_winding_t_0", "u_q_t_0")))
```

Several variables can be provided as evidence. In each prediction step, the values in t_0 of these variables will be fixed with the ones provided in f_dt_val. If we want to set an intervention and see its effect, we can do it as follows:

```{r forecast_ev_2}
inter_dt <- copy(f_dt_val)
inter_dt[, stator_winding_t_0 := 0.32]
res_fore <- suppressWarnings(dbnR::forecast_ts(inter_dt, fit, size, obj_var = c("pm_t_0"), ini = 100, len = 70, prov_ev = c("stator_winding_t_0", "u_q_t_0")))
```

There is no need to change 'stator_winding_t_1' and 'stator_winding_t_2', because on the next steps they will take the values fixed for t_0. This way, we can intervene multiple variables in the way we want and see the effects that it will have. The values are fixed in each time step, so we can make the interventions gradual increases, decreases or whatever we need them to do. As a simulation tool, this is very powerful. We have made some prototypes with shiny and in the future we intend to provide the capability to automatically generate a visual simulation interface with it.

## Some other functionality

Some functions of the package remain exported in case they can be useful elsewhere. Some examples of this functions are 'fold_dt', 'calc_mu' or 'calc_sigma. These last two are useful if you want to stick to the multivariate Gaussian representation of the network instead of the graph structure. The following example learns a network with some Gaussian noise added to the variables and prints the determinant of the sigma matrix:

```{r gaussian noise}
add_gauss_noise <- function(x, mu = NULL, sd = NULL){
  n <- length(x)
  if(is.null(mu))
    mu <- mean(x)
  if(is.null(sd))
    sd <- sd(x)
  
  noise <- rnorm(n, mu, sd)
  
  return(x + noise)
}

noisy_dt_train <- as.data.table(dt_train[, sapply(.SD,add_gauss_noise, mu= 0, sd = 2)])
net <- dbnR::learn_dbn_struc(noisy_dt_train, size, restrict = "mmpc", maximize = "hc",
                             restrict.args = list(test = "mi-g"),
                             maximize.args = list(score = "bge", maxp = 8))
noisy_f_dt_train <- fold_dt(dt_train, size)
fit <- dbnR::fit_dbn_params(net, noisy_f_dt_train, method = "mle")
print(paste0("The determinant of the noisy data is: ", det(calc_sigma(fit))))
```
